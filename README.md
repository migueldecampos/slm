# SLM
**S**mall **L**anguage **M**odels


## What is this?
This repo contains a from-scratch implementation of a transformer-based language model. This implementation allows training language models both on next-token and previous-token prediction. New features and upgrades will be made soon.


## Current features
* Training and inference scripts for forward (next token prediction) and reverse (previous token prediction) GPT2-like language models.
* Dataloaders for the TinyShakespeare and [TinyStories](https://arxiv.org/abs/2305.07759) datasets.


## Sources
Inspiered by and adapted from Andrej Karpathy's [video series](https://www.youtube.com/watch?v=kCc8FmEb1nY).
